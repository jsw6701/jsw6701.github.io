---
title: "[AI] 용어 정리"
excerpt: "시험기간 대비 용어 정리"

categories:
  - AI
tags:
  - [AI]

permalink: /ai/2

toc: true
toc_sticky: true
 
date: 2023-04-28
last_modified_at: 2023-04-28
---
# 지능설

- 퍼셉트론
    - 인간의 뇌의 뉴런을 모방하여 만든 것

### 활성화 함수란?

- 신경망에서 입력 신호의 총 합을 출력 신호로 변환하는 함수
- 퍼셉트론은 뉴런처럼 입력 받은 수치들을 계산하고, 출력하기 전에 활성화 함수 거쳐 출력에 변화 줌
- 종류
    - 계단 함수 : 특정한 값을 입력받았을 때, 값이 0이하 즉, 음수이면 무조건 0, 0초과이면 1

      ![1](https://jsw6701.github.io/assets/images/posts_img/230428/1.png)

      ![2](https://jsw6701.github.io/assets/images/posts_img/230428/2.png)


    - 시그모이드 함수 : S자형 곡선(시그모이드 곡선) 갖는 함수. 계단 함수의 단점을 보완
        - 0과 1의 실수로 구성 → 정교한 수를 전달 가능, 데이터 손실 감소
        - vanishing gradient 문제 발생 → 계층 많을수록 결국 gradient 값 0에 수렴
    
    ![3](https://jsw6701.github.io/assets/images/posts_img/230428/3.png)


※ vanishing gradient가 뭔가요?

딥러닝은 기울기 조정해 가며, 적절한 파라미터 값을 찾아 다니는데, 앞서 말한것 처럼 은닉층이 많게 되면, 1 미만의 값들이 계속 곱해지기 때문에, 결국 기울기가 0이 되어버리는 기울기 소멸 문제가 발생하게 됩니다. 이 문제가 바로 vanishing gradient입니다.

-
    - 하이퍼볼릭 탄젠트 함수 : 시그모이드 보완 함수
    -

![4](https://jsw6701.github.io/assets/images/posts_img/230428/4.png)

    ![5](https://jsw6701.github.io/assets/images/posts_img/230428/5.png)

        - -1 ~ 1 사이의 값을 출력
        - 차이점 : 평균 0.5가 아닌 0
        - 여전히 vanishing gradient 문제
    - ReLU 함수
        - 가장 많이 쓰이는 활성화 함수.
        - 0이상일 경우 그 수를 그대로 출력, 0 이하일 경우에는 0을 출력하는 함수.

          ![6](https://jsw6701.github.io/assets/images/posts_img/230428/6.png)

        - 0과 1사이 아니라 0 이하의 정보 과감히 무시
        - vanishing gradient 문제 해결
    - Softmax 함수
        - 그래프 존재 X, 결과 값을 확률로 바꿔주는 함수
        -

### MNIST

28X28 사이즈의 손으로 쓴 0~9까지의 숫자로 이루어진 데이터셋. 색상 채널이 없는 흑백 이미지

60000개의 훈련 세트와 10000개의 테스트 셋으로 구성

인수 : normalize, flatten, one_hot_label

### 순전파

입력층에서 은닉층 방향으로 이동하면서 각 입력에 해당하는 가중치가 곱해지고, 결과적으로 가중치 합으로 계산되어 은닉층 뉴런의 함수 값(일반적 sigmoid) 입력된다.

### 역전파

input, output 값을 알고 있는 상태에서 신경망 학습시키는 법. Supervised learnig(지도 학습)

초기 가중치, weight 값 랜덤. 각각 노드들은 하나의 퍼셉트론, 노드를 지날 때마다 활성함수 적용

### 에폭

한 번의 에폭은 인공 신경망에서 전체 데이터 셋에 대해 순방향 역방향 과정을 거친 것을 말함. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태

- 너무 크면 overfitting, 작으면 underfitting

### 배치사이즈

한 번의 배치마다 주는 데이터 샘플 사이즈

### 반복

1 에폭을 마치는데 필요한 미니배치 갯수.

**정리**

**만약 10,000개의 데이터셋을 학습시킨다고 치자. (여기서 학습은 순방향 역방향 둘 다 포함)**

**메모리 한계 및 성능을 고려하여 나눠서 학습을 시킬 겁니다.**

**이 때, 한 턴에 1,000개씩 10번 , 5턴을 학습시킨다고 하면, batch_size = 1,000 / iteration = 10 /epoch = 5입니다.**

